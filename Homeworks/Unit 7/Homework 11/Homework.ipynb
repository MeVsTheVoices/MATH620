{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d88620",
   "metadata": {},
   "source": [
    "# Unit 7 - Homework 10\n",
    "#### MATH620\n",
    "#### Joshua Dunne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06575283",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "### Find the singular value decomposition of the matrix\n",
    "$$\\mathbf{A}=\\begin{bmatrix}1 & 1 \\\\ 1 & 0 \\\\ 0 & 1\\end{bmatrix}$$\n",
    "\n",
    "I'm going to borrow substantially from something I wrote for the first question here, we want the SVD, but, we want to\n",
    "show the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4c0d848f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left * sigma * right: \n",
      "[[ 1.  1.]\n",
      " [ 1. -0.]\n",
      " [-0.  1.]]\n",
      "left_singular: \n",
      "[[ 1.  0.]\n",
      " [ 0. -1.]\n",
      " [ 0.  1.]]\n",
      "sigma: \n",
      "[[2. 0.]\n",
      " [0. 1.]]\n",
      "right_singular: \n",
      "[[ 1.  1.]\n",
      " [-1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([\n",
    "    [1,1],\n",
    "    [1,0],\n",
    "    [0,1]\n",
    "])\n",
    "\n",
    "a_transpose_a = A.T @ A\n",
    "\n",
    "# Hermitian, so, we can use eigh shortcut\n",
    "a_transpose_a_eigen_values, a_transpose_a_eigen_vectors = np.linalg.eigh(a_transpose_a)\n",
    "sort_indices = np.argsort(a_transpose_a_eigen_values)[::-1]\n",
    "# We want that these are ordered, and the corr. eigenvectors are likewise sorted\n",
    "sorted_eigenvalues = a_transpose_a_eigen_values[sort_indices]\n",
    "sorted_eigenvectors = a_transpose_a_eigen_vectors[:, sort_indices]\n",
    "# From those we can compose Sigma\n",
    "singular_values = np.sqrt(sorted_eigenvalues)\n",
    "sigma = np.diag(singular_values)\n",
    "# And compose our left singular\n",
    "left_singular = (A @ sorted_eigenvectors) / singular_values\n",
    "# And our right singular\n",
    "right_singular = sorted_eigenvectors.T\n",
    "# Lastly we check\n",
    "print(f\"left * sigma * right: \\n{np.around(left_singular @ sigma @ right_singular)}\")\n",
    "print(f\"left_singular: \\n{np.around(left_singular)}\")\n",
    "print(f\"sigma: \\n{np.around(sigma)}\")\n",
    "print(f\"right_singular: \\n{np.around(right_singular)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89edd56",
   "metadata": {},
   "source": [
    "That should surmise. The SVD is given by above. To summate, we find the eigenvectors of $\\mathbf{A}^T\\mathbf{A}$ to get $\\mathbf{V}$, the eigenvectors of $\\mathbf{A}\\mathbf{A}^T$ to get $\\mathbf{U}$, and the square roots of the eigenvalues of either to get $\\mathbf{\\Sigma}$. The only hangup here is getting the sorted order of the singular values, but that is easy enough to do. Lastly, we can just check against the built-in SVD function in numpy to verify our answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f65a1406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_singular: \n",
      "[[-1.  0. -1.]\n",
      " [-0. -1.  1.]\n",
      " [-0.  1.  1.]]\n",
      "sigma: \n",
      "[[2. 0.]\n",
      " [0. 1.]]\n",
      "right_singular: \n",
      "[[-1. -1.]\n",
      " [-1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"left_singular: \\n{np.around(np.linalg.svd(A)[0])}\")\n",
    "print(f\"sigma: \\n{np.around(np.diag(np.linalg.svd(A)[1]))}\")\n",
    "print(f\"right_singular: \\n{np.around(np.linalg.svd(A)[2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e8f4b4",
   "metadata": {},
   "source": [
    "There's some funk going on there, but, I'm happy with the right_singular and sigma matrices. The left_singular matrix seems to have some sign differences, but, that's not a problem since eigenvectors are only defined up to a sign."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1306ae",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "## Introduction\n",
    "We're given a few things that are worth listing first here. Given points $$(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)$$ we can find a linear function $y=c_0+c_1x$ from $$\\begin{bmatrix}1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n\\end{bmatrix}\\begin{bmatrix}c_0 \\\\ c_1\\end{bmatrix}=\\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\\end{bmatrix}$$ via least squares. This is done by solving the normal equations $$\\mathbf{A}^T\\mathbf{A}\\mathbf{c}=\\mathbf{A}^T\\mathbf{y}$$ where $\\mathbf{A}$ is the matrix on the left hand side above, $\\mathbf{c}$ is the vector of coefficients, and $\\mathbf{y}$ is the vector of $y$ values. If we substitue in the terms we can see the relationship more clearly: $$\\begin{bmatrix}n & \\sum x_i \\\\ \\sum x_i & \\sum x_i^2\\end{bmatrix}\\begin{bmatrix}c_0 \\\\ c_1\\end{bmatrix}=\\begin{bmatrix}\\sum y_i \\\\ \\sum x_iy_i\\end{bmatrix}$$ Using calculus we can do this another way. $$||r(c)||^2 = ||y - Ac||^2 = [y_1 - (c_0 + c_1 x_1)]^2 + \\dots + [y_m - (c_0 + c_1 x_m)]^2 = f(c_0, c_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a034c632",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "### Part a\n",
    "We want to show that if we set the partial derivatives equal to $0$ we get the same normal equations. We have two variables here we're actually interested in, $c_0$ and $c_1$. So we compute the partial derivatives with respect to each variable. I think we can just get by with throwing this in sympy, so, let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "81f73179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_0 derivative: 2*c0 + 2*c1*x - 2*y\n",
      "c_1 derivative: -2*x*(-c0 - c1*x + y)\n"
     ]
    }
   ],
   "source": [
    "from sympy import Matrix, symbols, diff, Sum, IndexedBase, Idx\n",
    "\n",
    "# We have four things we need\n",
    "c_0, c_1, x, y = symbols('c0 c1 x y')\n",
    "# From a function we can define as such\n",
    "error_term_squared = (y - (c_0 + c_1*x))**2\n",
    "# If we take the derivative of each\n",
    "d_c_0 = diff(error_term_squared, c_0)\n",
    "d_c_1 = diff(error_term_squared, c_1)\n",
    "print(f\"c_0 derivative: {d_c_0}\")\n",
    "print(f\"c_1 derivative: {d_c_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2716752d",
   "metadata": {},
   "source": [
    "We can restate the results a little more cleanly here. This was given in terms of a sum of all the possible points given, so, the derivative is better written for $c_0$ as $$\\frac{\\partial f}{\\partial c_0} = -2\\sum_{i=1}^m [y_i - (c_0 + c_1 x_i)]$$ and for $c_1$ as $$\\frac{\\partial f}{\\partial c_1} = -2\\sum_{i=1}^m [y_i - (c_0 + c_1 x_i)]x_i$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8957d8c2",
   "metadata": {},
   "source": [
    "We can afford a quick aside and do this symbolically. We want to set the partials equal to zero, so, let's use sympy and see where we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "41506dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_0 partials against 0: \\left\\{ c_{0} : - c_{1} x + y\\right\\}\n",
      "c_1 partials against 0: \\left\\{ c_{1} : \\frac{- c_{0} + y}{x}\\right\\}\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "critical_points_d_c_0 = sp.solve([d_c_0], [c_0])\n",
    "critical_points_d_c_1 = sp.solve([d_c_1], [c_1])\n",
    "print(f\"c_0 partials against 0: {sp.latex(critical_points_d_c_0)}\")\n",
    "print(f\"c_1 partials against 0: {sp.latex(critical_points_d_c_1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966bf76a",
   "metadata": {},
   "source": [
    "$$\\left\\{ c_{0} : - c_{1} x + y\\right\\}$$\n",
    "$$\\left\\{ c_{1} : \\frac{- c_{0} + y}{x}\\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f00387",
   "metadata": {},
   "source": [
    "We can write this back as the sum, as was intended. For $c_0$ we have $$\\frac{\\partial f}{\\partial c_0} = -2\\sum_{i=1}^m [y_i - (c_0 + c_1 x_i)] = 0$$ and for $c_1$ we have $$\\frac{\\partial f}{\\partial c_1} = -2\\sum_{i=1}^m [y_i - (c_0 + c_1 x_i)]x_i = 0$$ We can write than in terms of $c_0$ and $c_1$ as $$\\sum_{i=1}^m y_i = \\sum_{i=1}^m (c_0 + c_1 x_i)$$ and $$\\sum_{i=1}^m x_i y_i = \\sum_{i=1}^m (c_0 + c_1 x_i)x_i$$ Rearranging these gives us $$mc_0 + c_1\\sum_{i=1}^m x_i = \\sum_{i=1}^m y_i$$ and $$c_0\\sum_{i=1}^m x_i + c_1\\sum_{i=1}^m x_i^2 = \\sum_{i=1}^m x_i y_i$$ This is exactly the same as the normal equations we had before, so, we've shown what we wanted to show."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
