\documentclass{article}
\usepackage{fancyhdr} % for pretty formatting
\usepackage{amsmath} % for matrices
\usepackage{amssymb} % for bold text
\usepackage{pgfplots} % for graphs
\usepackage{hyperref} % for hyperlinks
\pgfplotsset{compat=1.18}
\usepackage{enumitem} % for custom lists

\usepackage{tikz}
\usepackage[svgnames]{xcolor}
\usetikzlibrary {positioning}
\definecolor {processblue}{cmyk}{0.96,0,0,0}

\usepackage{lipsum} % For dummy text
\usepackage{cite} % For citations

\pagestyle{fancy}  
\fancyhf{} % Clear all header and footer fields

\usepackage{tcolorbox} % Required for tcolorbox

\newtcolorbox{solutioncheck}{
    colback=green!10, % Background color
    colframe=gray!50, % Frame color
    boxsep=5pt, % Padding
    arc=4pt, % Rounded corners
    title=Checking solution, % Optional title for the aside
    fonttitle=\bfseries, % Title font style
} % for Asides

\lhead{Joshua Dunne}
\rhead{\thepage} % Displays the current page number 
\lfoot{MATH620}
\rfoot{Unit 4}
\cfoot{Homework 5}

\begin{document}
    \section{Question 1}
        \paragraph{Determine}
            Examine the transformations below and Determine
            whether they are linear. Justify this by the
            definition as given in class, showing that it does
            indeed hold, or showing where on which condition things
            break down.
        \subsection{Part a}
            $T: \mathbb{R}^n\rightarrow \mathbb{R}^n$
            by
            $T(\mathbf{x})=a\mathbf{x}+\mathbf{b}\quad \forall{b}\in{\mathbf{R}^{n\times n}}$
            \paragraph{Checking homogeneity}
                \subparagraph{Left hand side}
                    $T(c\mathbf{x})=\mathbf{a} (c \mathbf{x})+\mathbf{b}=c(\mathbf{ax})+\mathbf{b}$
                \subparagraph{Right hand side}
                    $c T(\mathbf{x})=c(\mathbf{ax}+\mathbf{b})=c(\mathbf{ax})+c\mathbf{b}$
                \subparagraph{Conclusion}
                    Homogeneity breaks as the two sides are not equivalent. The relationship
                    is not a linear transormation.
        \subsection{Part B}
            $T: \mathbb{R}^{n \times n} \rightarrow \mathbb{R}^{n \times n}$
            by
            $T(\mathbf{x})=\mathbf{Ax}-\mathbf{xA}$
            \paragraph{Checking homogeneity}
                \subparagraph{Left hand side}
                    $cT(\mathbf{x})=c(\mathbf{Ax}-\mathbf{xA})=c\mathbf{Ax}-c\mathbf{xA}$
                \subparagraph{Right hand side}
                    $T(c\mathbf{x})=\mathbf{A}(c\mathbf{x})-c\mathbf{xA}$
                \subparagraph{Conclusion}
                    Homogeneity checks out. We can bubble the c outwards and equate the two sides.
            \paragraph{Checking additivity}
                \subparagraph{Left hand side}
                    $T(\mathbf{u}+\mathbf{v})=\mathbf{A}(\mathbf{u}+\mathbf{v})-(\mathbf{u}+\mathbf{v})\mathbf{A}=\mathbf{Au}+\mathbf{Av}-\mathbf{uA}-\mathbf{vA}$
                \subparagraph{Right hand side}
                    \[(\mathbf{u})+T(\mathbf{v})=(\mathbf{Au}-\mathbf{uA})+(\mathbf{Av}-\mathbf{vA})=\mathbf{Au}+\mathbf{Av}-\mathbf{uA}-\mathbf{vA}\]
                \subparagraph{Conclusion}
                    Unlike above, additivity checks out. I've seen some other places have more requirements,
                    such as $\langle 0, 0, \dots \rangle \rightarrow \langle 0, 0, \dots \rangle$
                    but all of those follow from homogeneity and additivity. So, it is a linear transformation.
        \subsection{Part C}
            $\phi: \mathbb{C}\rightarrow \mathbb{C}$
            by
            $\phi(a+bi)=b+ai$
            and
            $c \in{\mathbb{C}}$
            where
            $c=d+ei$
            \paragraph{Checking homogeneity}
                \subparagraph{Left hand side}
                    $\phi(c[a+bi])=\phi(ca+bi)=(d+ei)(a+bi)=ad-be$
                \subparagraph{Right hand side}
                    $c\phi(a+bi)=c[a+bi]=(d+ei)(a+bi)=ca+bi$
                \subparagraph{Conclusion}
                    Homogeneity breaks. Having done this incorrectly before though,
                    with a non-complex scalar it will work. The relationship given
                    is not a linear transformation. I want to check additivity just out of curiosity.
            \paragraph{Examining additivity too}
                Let $c=d+ei$ and $f=g+hi$ as $c,\,f\in{\mathbb{C}}$.
                \subparagraph{Left hand side}
                    $\phi(c+f)=\phi([d+ei]+[g+hi])=\phi([d+g]+[e+h]i)=d+g-e-h$
                \subparagraph{Right hand side}
                    $\phi(c)+\phi(f)=\phi(d+ei)+\phi(g+hi)=e+di+h+gi$
                \subparagraph{Conclusion}
                    Guess it doesn't work here either. As $\phi(c+f)\ne\phi(c)+\phi(f)$\dots
                    well, atleast it isn't always true, the transformation isn't linear.
    \section{Question 2}
        Considering a matrix and finding $col(\mathbf{A}),\,row(\mathbf{A}),\,rank(\mathbf{A}),\,nul(\mathbf{A})$
        We're to develop a basis for each, and I'm going to recite definitions as we go so they're a little fresher.
        \paragraph{Given}
            First, let's RREF this bad boi, knock it's numbers around a little
        \[
            \mathbf{A}=\begin{bmatrix}3&2&4&4&0\\1&-1&3&9&11\\2&5&-1&-3&-13\\0&6&-6&4&-8\end{bmatrix}
            \rightarrow
            \begin{bmatrix}1&0&2&0&0\\0&1&-1&0&-2\\0&0&0&1&1\\0&0&0&0&0\end{bmatrix}
        \]
        \subsection[The column space of A]{$\text{col}(\mathbf{A})$}
            \paragraph{Definition}
                The column space is the set of all possible linear combinations of a matrices column vectors.
                \[
                    \text{col}(\mathbf{a})=\text{span}(\mathbf{c}_1,\mathbf{c}_2,\dots,\mathbf{c}_n)
                \]
            \paragraph{Basis}
                I feel that we could be cheaty for all these, as, simply giving each column a coefficient
                would still generate a the same space, however, if we look at the terms of the reduced matrix
                we see that only three rows actually start with a one. So, disreguarding the 3\textsuperscript{rd} and 5\textsuperscript{th}
                columns we can more efficiently generate the space with
                \[
                    \text{col}(\mathbf{A})=c_1\begin{bmatrix}3\\1\\2\\0\end{bmatrix}+c_2\begin{bmatrix}2\\-1\\5\\6\end{bmatrix}+c_3\begin{bmatrix}4\\9\\-3\\4\end{bmatrix}
                \]
        \subsection[The row space of A]{$\text{row}(\mathbf{A})$}
            \paragraph{Definition}
                The row space is the set of all possible linear combinations of a matrices row vectors.
                \[
                    \text{row}(\mathbf{A})=\text{span}(\mathbf{r}_1,\mathbf{r}_2,\dots,\mathbf{r}_n)
                \]
            \paragraph{Basis}
                Likewise as before, we can ditch those rows that have no leading ones.
                \[
                    \text{row}(\mathbf{A})=c_1\begin{bmatrix}3\\2\\4\\4\\0\end{bmatrix}+c_2\begin{bmatrix}1\\-1\\3\\9\\11\end{bmatrix}+c_3\begin{bmatrix}2\\5\\-1\\-3\\-13\end{bmatrix}
                \]
                I think I should be writing this a little differently, like,
                $basis_{\text{row}(\mathbf{A})}=\dots$ but that's a niggling issue for another day.
        \subsection[The column space of A]{$\text{column}(\mathbf{A})$}
            \paragraph{Definition}
                The column space is the set of all possible linear combinations of a matrices column vectors.
                \[
                    \text{row}(\mathbf{A})=\text{span}(\mathbf{c}_1,\mathbf{c}_2,\dots,\mathbf{c}_n)
                \]
            Ran in to a little uncertainty here that I plan to clarify sometime I get the time.
            The prevailing wisdom is that I should take only those original column vectors that
            have corresponding pivots in the reduced matrix. 
            We had pivots in the 1\textsuperscript{st}, 2\textsuperscript{nd}, and 4\textsuperscript{th} columns,
            so if we retain only those we can create a basis like so.
            \paragraph{Basis}
                \[
                    \text{col}(\mathbf{A})=c_1\begin{bmatrix}3\\1\\2\\0\end{bmatrix}+c_2\begin{bmatrix}2\\-1\\5\\6\end{bmatrix}+c_3\begin{bmatrix}4\\9\\-3\\4\end{bmatrix}
                \]
            We could have just as easily enumerated all the possible column vectors, but I'm reasonably confident
            in the way I'm interepreting the reduced form. Do need to double check and better understand why though.
        \subsection[The null space of A]{$\text{null}(\mathbf{A})$}
            \paragraph{Definition}
                The null space is the of all possible linear combinations that, when multiplied by the original
                matrix, result in the zero vector
                \[
                    \text{null}(\mathbf{A})=\{\mathbf{x}\,|\,\mathbf{Ax}=\mathbf{0}\}
                \]
            Starting with our reduced form, we can augment said matrix with the zero vector and solve
            \paragraph{Solution}
                We imagine a column of zero's along the right side of the previous. We have two free, so, choose
                $x_3=0$ and $x_5=1$ so that we don't end up with a trivial solution. Doing this we get\dots$x_1=0$, $x_2=2$, $x_4=-1$
                We still need one more though, with two free variables. So, for the other, let $x_3=1$ and $x_5=0$.
                We get $x_4=0$, $x_1=-2$, $x_2=1$.
                
            \paragraph{Basis}
                \[
                    \text{null}(\mathbf{A})=c_1\begin{bmatrix}1\\2\\0\\-1\\1\end{bmatrix}+c_2\begin{bmatrix}-2\\1\\1\\0\\0\end{bmatrix}
                \]
        \subsection[The rank of A]{$\text{rank}(\mathbf{A})$}
            We've already done all the heavy lifting here. We know that $\text{rank}(\mathbf{A})=3$ from way back.
            It was the number of pivots in our reduced form.
    \section{Question 3}
        We want to find all possible values of a matrix $\mathbf{A}$ such that $\text{rank}(\mathbf{A})=3$ and that $\text{rank}(\mathbf{A})=2$.
        \paragraph{Given}
            \[
                \mathbf{A}=\begin{bmatrix}1&2&\alpha\\4&4&8\\\alpha&1&4\end{bmatrix}
            \]
        \paragraph{Finding where rank is not 3}
            Let's jump in and take the determinant to start, this should give us a relationship we can work with.
            \[
                \text{det}(\mathbf{A})=\alpha(16-4\alpha)-1(8-4\alpha)+4(4-8)=-4\alpha^2+20\alpha-24
            \]
            When the determinant is $0$, we can't be spanning all of $\mathbb{R}^3$ so
            \[
                -4\alpha^2+20\alpha-24=0
                \rightarrow
                \alpha^2-5\alpha+6=0
            \]
            So, we have solutions where $\alpha=2$ and $\alpha=3$. If either of these is true, we do not span $R^3$
            \subparagraph{Fining where the rank is 2}
                So, we know we don't span $\mathbb{R}^3$ is $\alpha=3$ or $\alpha=2$. Now we need to figure out which
                of these might limit our span even down to $\mathbb{R}$. I'm going to borrow a trick from Jack and make this simple.
                Because we have a 2x2 submatrix whose determinant is nonzero, $$det(\begin{bmatrix}1&2\\4&4\end{bmatrix})\ne0$$
                For both values $\alpha=2$ and $\alpha=3$, the $\text{span}(\mathbf{A})=\mathbb{R}^2$
        \paragraph{Finiding where the rank is 3}
            Likewise with before, we've already done all the work there. There are only $2$ values of $\alpha$
            that will lead to nonzero determinants, therefor any value for which $\alpha \ne 2$ and $\alpha \ne 3$
            the $\text{span}(\mathbf{A})=\mathbb{R}^3$.
    \section{Question 4}
        We want the $\text{null}(\mathbf{A})$ and $\text{null}(\mathbf{B})$ 
            \paragraph{Given}
                \[
                    \mathbf{A}=\begin{bmatrix}0&0&0&0\\0&0&0&0\end{bmatrix}
                    \text{, }
                    \mathbf{B}=\begin{bmatrix}3&6&3&6\\1&2&1&2\end{bmatrix}
                \]
            \paragraph{For $\mathbf{A}$}
                This looks a little awkward, but, we're still after the same thing
                \[
                    \begin{bmatrix}0&0&0&0\\0&0&0&0\end{bmatrix}
                    \begin{bmatrix}x_1\\x_2\\x_3\\x_4\end{bmatrix}
                    =
                    \begin{bmatrix}0\\0\end{bmatrix}
                \]
                It should be easy to see that any values $x_1$, $x_2$, $x_3$, $x_4$ in $\mathbb{R}$
                will satisfy this equation. So, we can write the basis for the null space as
                \[
                    \text{null}(\mathbf{A})=c_1\begin{bmatrix}1\\0\\0\\0\end{bmatrix}+c_2\begin{bmatrix}0\\1\\0\\0\end{bmatrix}+c_3\begin{bmatrix}0\\0\\1\\0\end{bmatrix}+c_4\begin{bmatrix}0\\0\\0\\1\end{bmatrix}
                    =
                    \mathbf{I}_4
                \]
            \paragraph{For $\mathbf{B}$}
                Ew. This looks nasty, but, we can row reduce it and make it a little easier.
                \[
                    \begin{bmatrix}3&6&3&6\\1&2&1&2\end{bmatrix}
                    \rightarrow
                    \begin{bmatrix}1&2&1&2\\0&0&0&0\end{bmatrix}
                \]
                Associating this with the corresponding vector we get\dots
                \[
                    x_1+2x_2+x_3+2x_4=0
                \]
                We could totally just leave things here, we're saying that you can pick
                anything you want for the last three, and that only the first has to be determined.
                Anything that satisfies above will solve $\mathbf{Bx}=\mathbf{0}$.
                Continuing on though, we can do a little rearranging.
                We have three free variables, and this kind of flies in the face of intuition,
                but we can follow the methodology presented in class. If we specify the relationships for each $x_n$
                in terms of the free variables, taking $x_2$, $x_3$, and $x_4$ as free, we get\dots
                \[
                    \mathbf{x}
                    =
                    \begin{bmatrix}x_1\\x_2\\x_3\\x_4\end{bmatrix}
                    =
                    \begin{bmatrix}-2x_2-x_3-2x_4\\x_2\\x_3\\x_4\end{bmatrix}
                    =
                        x_2\begin{bmatrix}-2\\1\\0\\0\end{bmatrix}
                        +
                        x_3\begin{bmatrix}-1\\0\\1\\0\end{bmatrix}
                        +
                        x_4\begin{bmatrix}-2\\0\\0\\1\end{bmatrix}
                \]
                How's that for nasty? We have so many free variables we can write
                the null space in terms of the other variables. This is both our nullspace
                \textbf{and} the basis for our nullspace\dots
    \section{Question 5}
        Starting with a $4\times 8$, and adding as many ones as possible so that
        $x_2$, $x_4$, $x_5$, and $x_6$ are free.
        \[
            \begin{bmatrix}0&0&0&0&0&0&0&0\\0&0&0&0&0&0&0&0\\0&0&0&0&0&0&0&0\\0&0&0&0&0&0&0&0\end{bmatrix}
            \begin{bmatrix}x_1\\x_2\\x_3\\x_4\\x_5\\x_6\\x_7\\x_8\end{bmatrix}
        \]
        \paragraph{Restrictions}
            We know that we can't have pivots in columns for those entires.
            However, we do have to have pivots for those variables that aren't free.
            Let's do the latter first, then we can pad right.
            Also, because we're assuming RREF'ed, nix anything in the lower diagonal.
        \paragraph{With pivots}
        \[
        \begin{bmatrix}1&0&0&0&0&0&0&0\\0&0&1&0&0&0&0&0\\0&0&0&0&0&0&1&0\\0&0&0&0&0&0&0&1\end{bmatrix}
        \]
        \paragraph{Padding right}
            From there, we have considerable freedom, \textbf{if} we have a pivot in that column,
            then we \textbf{don't} have a 1 anywhere else.
            \[
                \begin{bmatrix}1&1&0&1&1&1&0&0\\0&0&1&1&1&1&0&0\\0&0&0&0&0&0&1&0\\0&0&0&0&0&0&0&1\end{bmatrix}
            \]
            That should satisfy. I can't think where else I could place a one and not break anything else.
    \section{Question 6}
        \paragraph{Depiction}
            \begin {center}
            \begin {tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid ,
                semithick ,
                state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
                draw,processblue , text=blue , minimum width =1 cm}]
                \node[state] (A){$1$};
                \node[state] (B) [right = of A]{$2$};
                \node[state] (D) [below = of A, xshift=2.5cm] {$4$};
                \node[state] (C) [below = of D]{$3$};
                \path (A) edge node[above] {$y_1$} (B);
                \path (B) edge node[above left] {$y_5$} (D);
                \path (C) edge node[left] {$y_6$} (D);
                \path (A) edge node[above right] {$y_4$} (D);
                \path (C) edge node[above left] {$y_3$} (A);
                \path (B) edge node[above right] {$y_2$} (C);

            \end{tikzpicture}
            \end{center}
            This looks familiar, let's take a column of zero's as our augment, then we can equate
        \paragraph{Analysis}
            \begin{align*}
                0&=-y_1+y_3+y_4\\
                0&=y_1-y_2-y_5\\
                0&=y_2-y_3-y_6\\
                0&=y_4+y_5+y_6
            \end{align*}
            \[
                \begin{bmatrix}
                    -1 &  0 &  1 &  1 &  0 &  0 \\
                     1 & -1 &  0 &  0 & -1 &  0 \\
                     0 &  1 & -1 &  0 &  0 & -1 \\
                     0 &  0 &  0 &  1 &  1 &  1 \\
                \end{bmatrix}
                \begin{bmatrix}
                    y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5 \\ y_6
                \end{bmatrix}
                =
                \begin{bmatrix}
                    0 \\ 0 \\ 0 \\ 0
                \end{bmatrix}
            \]
            We're after the null space of this matrix, so, let's reduce.
            \[
                \begin{bmatrix}
                    1 & 0 & -1 & 0 & 0 & 0 \\
                    0 & 1 & -1 & 0 & 0 & -1 \\
                    0 & 0 & 0 & 1 & 0 & 0 \\
                    0 & 0 & 0 & 0 & 1 & 1 
                \end{bmatrix}
            \]
            Ok, we're missing pivots in\dots the 3\textsuperscript{rd}, 5\textsuperscript{th}, and 6\textsuperscript{th} columns.
            So, we can write the relationships for those in terms of the other variables.
            \begin{align*}
                y_1&=y_3-y_4\\
                y_2&=y_3+y_6\\
                y_3&=y_3\\
                y_4&=y_4\\
                y_5&=-y_6-y_4\\
                y_6&=y_6
            \end{align*}
            And this should form the basis for our null space, we can take
            a linear combination of the free variables to produce\dots
            \[
                \text{null}(\mathbf{A})=
                c_1\begin{bmatrix}1\\1\\1\\0\\0\\0\end{bmatrix}+
                c_2\begin{bmatrix}-1\\0\\0\\1\\-1\\0\end{bmatrix}+
                c_3\begin{bmatrix}0\\1\\0\\0\\-1\\1\end{bmatrix}
            \]
            Meaning the basis for the null space.
\end{document}